# [FR-004] RAG Vectorization & Retrieval Engine

## Labels
`MUS`, `enhancement`, `rag`, `backend`

## User Story
As the system, I need an automated background process to transform long curriculum documents into semantic vectors so the AI can retrieve strictly relevant context.

## Proposed Solution

### Overview
Build a processing pipeline that activates after an upload (`FR-003`). The service will take raw document text, break it into smaller chunks, map each chunk to an embedding using `OpenAI` or `Cohere` embeddings (via OpenRouter equivalent or direct API, e.g., `text-embedding-3-small`), and insert the vectors into Neon (Postgres `pgvector`).

### Implementation Flow
1. Install necessary dependencies: `@pinecone-database/pinecone` (if shifting from Neon) OR raw SQL query handlers for `pgvector`, and `ai` (Vercel AI SDK core).
2. Create a `knowledge.service.ts`.
3. Implement text chunking (e.g., using `recursive-character-text-splitter` from LangChain).
4. Implement vectorization using the AI SDK's `embedMany` function.
5. Create a Prisma raw SQL insertion method for `DocumentEmbedding` that handles the mapping of the `Prisma.Unsupported('vector')` type.
6. Create an equivalent search function: `findSimilarContext(query: string) -> string[]`.

### Technical Approach

```typescript
// Proposed Server Service Implementation
import { embedMany } from "ai";
import { openai } from "@ai-sdk/openai"; // Or another embedding provider
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import prisma from "@/lib/prisma";

export async function processDocumentForRAG(documentId: number, rawText: string) {
    // 1. Chunk
    const splitter = new RecursiveCharacterTextSplitter({
        chunkSize: 1000,
        chunkOverlap: 200,
    });
    
    const chunks = await splitter.createDocuments([rawText]);
    const chunkTexts = chunks.map(c => c.pageContent);

    // 2. Embed
    const { embeddings } = await embedMany({
        model: openai.embedding('text-embedding-3-small'),
        values: chunkTexts,
    });

    // 3. Store in Postgres using raw SQL due to pgvector Prisma limitations
    for (let i = 0; i < chunks.length; i++) {
        const text = chunkTexts[i];
        const embedding = embeddings[i];
        // Note: The precise syntax depends on your Neon/pgvector setup
        await prisma.$executeRaw`
            INSERT INTO "DocumentEmbedding" ("documentId", "contentChunk", "embedding")
            VALUES (${documentId}, ${text}, ${embedding}::vector)
        `;
    }
}
```

### Key Considerations
- Text splitting strategy is vital. Overlaps are needed to preserve sentence meaning.
- Prisma has historically had slightly awkward support for `pgvector`; verify if native support exists in Prisma 5+ or if raw queries via `$executeRaw` are required.
- Cost: Ensure we are using a fast, low-cost embedding model, as curriculum PDFs can be massive.

## Acceptance Criteria
- [ ] Raw text can be ingested and chunked correctly.
- [ ] Chunks are assigned vector embeddings via an API layer.
- [ ] Vectors are stored securely in the Postgres database.
- [ ] A retrieval function successfully queries the database for the nearest vector neighbors based on a given string and returns the raw `contentChunk`.
