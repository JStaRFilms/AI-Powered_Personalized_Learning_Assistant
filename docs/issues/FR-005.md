# [FR-005] Adaptive AI Chat Interface

## Labels
`MUS`, `enhancement`, `chat`, `frontend`

## User Story
As a student, I want to chat with an AI tutor that answers my questions based on my uploaded materials and adjusts to my current mastery level.

## Proposed Solution

### Overview
Build a ChatGPT-like web interface using Tailwind CSS and the Vercel AI SDK (`useChat`). The backend route handler will orchestrate the LLM call via OpenRouter, perform the RAG retrieval (`FR-004`) to find relevant context, and optionally track topic mastery (`FR-006` Future).

### Implementation Flow
1. Create a responsive `ChatInterface` client component with `useChat` from `@ai-sdk/react`.
2. Implement the `app/api/chat/route.ts` using the Vercel AI SDK to stream the response.
3. In the route handler, extract the user's latest query.
4. Pass the query to the `knowledge.service.ts` to retrieve the relevant `DocumentEmbedding` chunks from Neon.
5. Retrieve the student's mastery level from Prisma (default: "Beginner" if no context exists).
6. Construct the `system` prompt: Inject the "Tutor Persona", the retrieved text chunks, and the instruction to tailor the response to the user's mastery level.
7. Call the OpenRouter model (e.g., `openai/gpt-4o` or `anthropic/claude-3.5-sonnet`) using `@openrouter/ai-sdk-provider`.
8. Stream the response back to the client.

### Technical Approach

```tsx
// Frontend Component (Simplified)
'use client'
import { useChat } from 'ai/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();
  return (
    <div className="flex flex-col h-screen p-4">
      {messages.map(m => (
        <div key={m.id} className={m.role === 'user' ? 'user-msg' : 'ai-msg'}>
          {m.content}
        </div>
      ))}
      <form onSubmit={handleSubmit} className="mt-auto">
        <input value={input} onChange={handleInputChange} className="w-full input-styling" />
      </form>
    </div>
  );
}

// Backend Route Handler (app/api/chat/route.ts)
import { openrouter } from '@openrouter/ai-sdk-provider';
import { streamText } from 'ai';
import { findSimilarContext } from '@/lib/knowledge.service'; 

export async function POST(req: Request) {
  const { messages } = await req.json();
  const latestMessage = messages[messages.length - 1].content;

  // RAG Pipeline
  const contextChunks = await findSimilarContext(latestMessage);
  const contextString = contextChunks.join("\n\n");

  const systemPrompt = `You are a helpful academic tutor. 
  Answer the student's question strictly using the provided context. If the answer is not in the context, say "I don't know based on your materials."
  
  CONTEXT:
  ${contextString}`;

  const result = streamText({
    model: openrouter('openai/gpt-4o'), // User configurable
    system: systemPrompt,
    messages,
  });

  return result.toDataStreamResponse();
}
```

### Key Considerations
- Ensure the prompt strictly enforces the RAG constraint to prevent hallucinations.
- OpenRouter requires a specific provider setup with the AI SDK (`@openrouter/ai-sdk-provider`).
- The UI must handle streaming smoothly, rendering markdown correctly.

## Acceptance Criteria
- [x] The user can send a message in a clean, responsive chat UI.
- [x] The backend retrieves relevant context from the database for the query.
- [x] The AI responds strictly based on the retrieved context, streaming the answer back to the UI.
- [x] Past messages in the current session are visible.
